{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, time, json, ast, re, uuid, random\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Qdrant imports\n",
        "try:\n",
        "    from qdrant_client import QdrantClient\n",
        "    from qdrant_client.http.models import Distance, VectorParams, PayloadSchemaType, Filter, FieldCondition, Range\n",
        "except Exception:\n",
        "    from qdrant_client import QdrantClient\n",
        "    from qdrant_client.models import Distance, VectorParams, PayloadSchemaType, Filter, FieldCondition, Range  # fallback\n",
        "\n",
        "# -------------------- USER CONFIG --------------------\n",
        "CSV_PATH = \"/content/drive/MyDrive/DATA/fda_maude_anomaly_ready.csv\"\n",
        "COLLECTION_NAME = \"fda_maude_rag\"\n",
        "QDRANT_URL = \"https://20851a9b-65fb-47d0-982e-38fdfc7d76f8.europe-west3-0.gcp.cloud.qdrant.io\"\n",
        "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.ooylH9ScdhcxPm0MywCTUSBDULcNCuuL4U8wd52UwXY\"                                          # <-- required\n",
        "\n",
        "ID_NAMESPACE = uuid.UUID(\"8c6b1d0d-7a8c-4c50-bc2e-7e4d5f6a1b23\")\n",
        "\n",
        "ID_COLS_FOR_UUID = [\n",
        "    \"mdr_report_key\", \"device_sequence_no\", \"report_number\",\n",
        "    \"date_received\", \"model_number\", \"row_in_group\"\n",
        "]\n",
        "\n",
        "EMBED_COL = \"foi_embedding\"\n",
        "VECTOR_NAME = \"foi_embedding\"\n",
        "UPSERT_BATCH = 1000\n",
        "VERIFY_BATCH = 1000\n",
        "RETRY_TIMES = 3\n",
        "RETRY_BACKOFF = 2.0\n",
        "FILL_MISSING_EMBEDDINGS = True\n",
        "\n",
        "# Columns to upload (minimal payload + derived date fields)\n",
        "PAYLOAD_COLS = [\n",
        "    \"mdr_report_key\", \"report_number\",\n",
        "    \"date_received\",\n",
        "    \"year\", \"date_int\",\n",
        "    \"brand_name\", \"generic_name\", \"manufacturer_d_name\",\n",
        "    \"event_type\", \"adverse_event_flag\", \"product_problem_flag\",\n",
        "    \"model_number\",\n",
        "    \"foi_text\",\n",
        "    \"anomaly_score\", \"reconstruction_error\", \"ae_anomaly\", \"anomaly_flag\",\n",
        "\n",
        "]\n",
        "# -----------------------------------------------------\n",
        "\n",
        "client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
        "print(\"Connected to Qdrant:\", QDRANT_URL)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuXvh5OSFVY5",
        "outputId": "2e6170fa-c489-4bc9-cbee-d593e2c7a3c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to Qdrant: https://20851a9b-65fb-47d0-982e-38fdfc7d76f8.europe-west3-0.gcp.cloud.qdrant.io\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def parse_embedding(val):\n",
        "    \"\"\"Robustly parse embedding: JSON list, python list, array(...), comma/semicolon/space separated.\"\"\"\n",
        "    if val is None:\n",
        "        return None\n",
        "    if isinstance(val, (list, tuple, np.ndarray)):\n",
        "        try:\n",
        "            return [float(x) for x in val]\n",
        "        except Exception:\n",
        "            return None\n",
        "    s = str(val).strip()\n",
        "    if not s or s.lower() in {\"none\", \"nan\"}:\n",
        "        return None\n",
        "    if s.startswith(\"array(\") and s.endswith(\")\"):\n",
        "        s = s[len(\"array(\"):-1].strip()\n",
        "\n",
        "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
        "        try:\n",
        "            arr = json.loads(s)\n",
        "            if isinstance(arr, list):\n",
        "                return [float(x) for x in arr]\n",
        "        except Exception:\n",
        "            try:\n",
        "                arr = ast.literal_eval(s)\n",
        "                if isinstance(arr, (list, tuple)):\n",
        "                    return [float(x) for x in arr]\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    if \";\" in s:\n",
        "        try:\n",
        "            return [float(x) for x in s.split(\";\") if x.strip()]\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    s2 = s.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \").strip(\"[]\")\n",
        "    if \",\" in s2:\n",
        "        try:\n",
        "            return [float(x) for x in s2.split(\",\") if x.strip()]\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    parts = re.split(r\"\\s+\", s2)\n",
        "    try:\n",
        "        vec = [float(x) for x in parts if x]\n",
        "        return vec if len(vec) > 1 else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def coerce_payload(d: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Replace NaN with None; cast numpy types to Python primitives.\"\"\"\n",
        "    out = {}\n",
        "    for k, v in d.items():\n",
        "        if isinstance(v, float) and (np.isnan(v) or np.isinf(v)):\n",
        "            out[k] = None\n",
        "        elif pd.isna(v):\n",
        "            out[k] = None\n",
        "        elif isinstance(v, (np.integer,)):\n",
        "            out[k] = int(v)\n",
        "        elif isinstance(v, (np.floating,)):\n",
        "            out[k] = float(v)\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def chunker(seq, size):\n",
        "    for i in range(0, len(seq), size):\n",
        "        yield seq[i:i+size]\n",
        "\n",
        "def upsert_batch(points):\n",
        "    last_err = None\n",
        "    for attempt in range(1, RETRY_TIMES + 1):\n",
        "        try:\n",
        "            client.upsert(collection_name=COLLECTION_NAME, points=points, wait=True)\n",
        "            return\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            sleep_s = RETRY_BACKOFF ** attempt\n",
        "            print(f\"Upsert retry {attempt}/{RETRY_TIMES} after error: {e.__class__.__name__}: {e}. Sleeping {sleep_s:.1f}s...\")\n",
        "            time.sleep(sleep_s)\n",
        "    raise last_err\n"
      ],
      "metadata": {
        "id": "xRm0dzMeFlxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Loading CSV:\", CSV_PATH)\n",
        "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
        "print(\"Rows:\", len(df), \" Columns:\", len(df.columns))\n",
        "\n",
        "# --- Standardize date field ---\n",
        "if \"date_received\" in df.columns:\n",
        "    df[\"date_received_std\"] = df[\"date_received\"]\n",
        "elif \"date_recevied\" in df.columns:\n",
        "    df[\"date_received_std\"] = df[\"date_recevied\"]\n",
        "else:\n",
        "    raise ValueError(\"Neither 'date_received' nor 'date_recevied' found in CSV.\")\n",
        "\n",
        "\n",
        "dr = pd.to_datetime(df[\"date_received_std\"], errors=\"coerce\")\n",
        "df[\"year\"] = dr.dt.year.astype(\"Int64\")\n",
        "df[\"date_int\"] = pd.to_numeric(dr.dt.strftime(\"%Y%m%d\"), errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "\n",
        "df[\"date_received\"] = df[\"date_received_std\"]\n",
        "\n",
        "\n",
        "sort_cols = [c for c in [\"mdr_report_key\", \"device_sequence_no\", \"date_received\", \"report_number\", \"model_number\"] if c in df.columns]\n",
        "df = df.sort_values(sort_cols, kind=\"mergesort\").reset_index(drop=True)\n",
        "\n",
        "\n",
        "grp_cols = [c for c in [\"mdr_report_key\", \"device_sequence_no\"] if c in df.columns]\n",
        "if len(grp_cols) < 1:\n",
        "\n",
        "    grp_cols = [\"mdr_report_key\"]\n",
        "df[\"row_in_group\"] = df.groupby(grp_cols, dropna=False).cumcount()\n",
        "\n",
        "\n",
        "def make_row_uid(row) -> str:\n",
        "    parts = []\n",
        "    for k in ID_COLS_FOR_UUID:\n",
        "\n",
        "        val = row[k] if k in row and pd.notna(row[k]) else \"<NA>\"\n",
        "        parts.append(str(val))\n",
        "    key = \"|\".join(parts)\n",
        "    return str(uuid.uuid5(ID_NAMESPACE, key))\n",
        "\n",
        "\n",
        "for k in ID_COLS_FOR_UUID:\n",
        "    if k not in df.columns:\n",
        "        df[k] = \"<NA>\"\n",
        "df[\"point_id\"] = df.apply(make_row_uid, axis=1)\n",
        "assert df[\"point_id\"].nunique() == len(df), \"point_id not unique — check ID_COLS_FOR_UUID / row_in_group logic.\"\n",
        "\n",
        "print(\"✅ Deterministic UUIDs generated for all rows.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nvNhzQsFsbB",
        "outputId": "3c9d375d-c582-46bc-8269-d85eaa0241b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CSV: /content/drive/MyDrive/DATA/fda_maude_anomaly_ready.csv\n",
            "Rows: 462682  Columns: 50\n",
            "✅ Deterministic UUIDs generated for all rows.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "keep_cols = sorted(set(PAYLOAD_COLS + [EMBED_COL, \"point_id\"]))\n",
        "missing_cols = [c for c in keep_cols if c not in df.columns]\n",
        "if missing_cols:\n",
        "    raise ValueError(f\"Missing required columns for upload: {missing_cols}\")\n",
        "df_up = df[keep_cols].copy()\n",
        "\n",
        "\n",
        "vec_size = None\n",
        "for v in df_up[EMBED_COL]:\n",
        "    parsed = parse_embedding(v)\n",
        "    if parsed:\n",
        "        vec_size = len(parsed)\n",
        "        break\n",
        "if vec_size is None:\n",
        "    raise ValueError(\"Could not parse any embeddings. Check EMBED_COL content/format.\")\n",
        "print(\"Detected embedding size:\", vec_size)\n",
        "\n",
        "default_vec = [0.0] * vec_size if FILL_MISSING_EMBEDDINGS else None\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODThhfC3Ft-T",
        "outputId": "7b64d8d8-6c8e-438f-c843-8204659d8c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected embedding size: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "existing = [c.name for c in client.get_collections().collections]\n",
        "if COLLECTION_NAME not in existing:\n",
        "    print(\"Creating collection:\", COLLECTION_NAME)\n",
        "    client.create_collection(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        vectors_config={VECTOR_NAME: VectorParams(size=vec_size, distance=Distance.COSINE)}\n",
        "    )\n",
        "else:\n",
        "    print(\"Using existing collection:\", COLLECTION_NAME)\n",
        "\n",
        "\n",
        "INDEX_FIELDS = [\n",
        "    (\"brand_name\", PayloadSchemaType.KEYWORD),\n",
        "    (\"generic_name\", PayloadSchemaType.KEYWORD),\n",
        "    (\"manufacturer_d_name\", PayloadSchemaType.KEYWORD),\n",
        "    (\"event_type\", PayloadSchemaType.KEYWORD),\n",
        "    (\"adverse_event_flag\", PayloadSchemaType.KEYWORD),\n",
        "    (\"product_problem_flag\", PayloadSchemaType.KEYWORD),\n",
        "    (\"model_number\", PayloadSchemaType.KEYWORD),\n",
        "    (\"year\", PayloadSchemaType.INTEGER),\n",
        "    (\"date_int\", PayloadSchemaType.INTEGER),\n",
        "]\n",
        "for field, schema in INDEX_FIELDS:\n",
        "    try:\n",
        "        try:\n",
        "            client.create_payload_index(COLLECTION_NAME, field, schema)\n",
        "        except TypeError:\n",
        "            client.create_payload_index(collection_name=COLLECTION_NAME, field_name=field, field_schema=schema)\n",
        "        print(f\"Index ensured on '{field}'.\")\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtmPcGfXGETG",
        "outputId": "17c64fec-63b7-4386-8752-cadf805fd4a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating collection: fda_maude_rag\n",
            "Index ensured on 'brand_name'.\n",
            "Index ensured on 'generic_name'.\n",
            "Index ensured on 'manufacturer_d_name'.\n",
            "Index ensured on 'event_type'.\n",
            "Index ensured on 'adverse_event_flag'.\n",
            "Index ensured on 'product_problem_flag'.\n",
            "Index ensured on 'model_number'.\n",
            "Index ensured on 'year'.\n",
            "Index ensured on 'date_int'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_point(row: pd.Series):\n",
        "    vec = parse_embedding(row[EMBED_COL])\n",
        "    if vec is None:\n",
        "        if default_vec is None:\n",
        "            return None\n",
        "        vec = default_vec\n",
        "    payload = {c: row.get(c, None) for c in PAYLOAD_COLS if c in row}\n",
        "    payload = coerce_payload(payload)\n",
        "    return {\n",
        "        \"id\": row[\"point_id\"],            # UUIDv5 string id\n",
        "        \"vector\": {VECTOR_NAME: vec},\n",
        "        \"payload\": payload,\n",
        "    }\n",
        "\n",
        "print(\"Starting upload...\")\n",
        "skipped = 0\n",
        "total_rows = len(df_up)\n",
        "pbar = tqdm(total=total_rows, desc=\"Upserting\", unit=\"rows\")\n",
        "\n",
        "for idx_chunk in chunker(list(range(total_rows)), UPSERT_BATCH):\n",
        "    points = []\n",
        "    for i in idx_chunk:\n",
        "        pt = build_point(df_up.iloc[i])\n",
        "        if pt is None:\n",
        "            skipped += 1\n",
        "            continue\n",
        "        points.append(pt)\n",
        "    if points:\n",
        "        upsert_batch(points)\n",
        "    pbar.update(len(idx_chunk))\n",
        "\n",
        "pbar.close()\n",
        "print(f\"Upload complete. Skipped (missing embeddings without fill): {skipped}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkdJdk_WGGjM",
        "outputId": "812336d4-9737-4896-de3c-fc144dcc89fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting upload...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Upserting: 100%|██████████| 462682/462682 [24:20<00:00, 316.72rows/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload complete. Skipped (missing embeddings without fill): 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Verify counts\n",
        "try:\n",
        "    q_count = client.count(collection_name=COLLECTION_NAME, exact=True).count\n",
        "    print(f\"Qdrant count: {q_count} | CSV rows: {len(df_up)}\")\n",
        "except Exception as e:\n",
        "    print(\"Count check failed:\", e)\n",
        "\n",
        "# Reconcile missing IDs (fetch in chunks and re-upsert if needed)\n",
        "ids = df[\"point_id\"].tolist()\n",
        "\n",
        "def verify_and_reconcile():\n",
        "    missing = []\n",
        "    for id_chunk in tqdm(list(chunker(ids, VERIFY_BATCH)), desc=\"Verifying IDs\", unit=\"ids\"):\n",
        "        try:\n",
        "            found = client.retrieve(collection_name=COLLECTION_NAME, ids=id_chunk, with_payload=False, with_vectors=False)\n",
        "            found_ids = {p.id for p in found}\n",
        "            for _id in id_chunk:\n",
        "                if _id not in found_ids:\n",
        "                    missing.append(_id)\n",
        "        except Exception as e:\n",
        "            print(\"Retrieve error; will continue:\", e)\n",
        "            time.sleep(1)\n",
        "\n",
        "    print(\"Missing IDs after first pass:\", len(missing))\n",
        "\n",
        "    if missing:\n",
        "        # Re-upsert missing\n",
        "        for miss_chunk in tqdm(list(chunker(missing, UPSERT_BATCH)), desc=\"Re-upserting missing\", unit=\"ids\"):\n",
        "            sub = df_up[df[\"point_id\"].isin(miss_chunk)]\n",
        "            pts = []\n",
        "            for _, row in sub.iterrows():\n",
        "                pt = build_point(row)\n",
        "                if pt is not None:\n",
        "                    pts.append(pt)\n",
        "            if pts:\n",
        "                upsert_batch(pts)\n",
        "\n",
        "        # Re-check\n",
        "        still = []\n",
        "        for id_chunk in chunker(missing, VERIFY_BATCH):\n",
        "            found = client.retrieve(collection_name=COLLECTION_NAME, ids=id_chunk, with_payload=False, with_vectors=False)\n",
        "            found_ids = {p.id for p in found}\n",
        "            still.extend([_id for _id in id_chunk if _id not in found_ids])\n",
        "        print(\"Still missing after retry:\", len(still))\n",
        "    else:\n",
        "        print(\"No missing IDs detected.\")\n",
        "\n",
        "verify_and_reconcile()\n",
        "print(\"Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGL9-4oKGJdk",
        "outputId": "c79deac2-61e4-4870-eb38-a5a1b94ed587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qdrant count: 462682 | CSV rows: 462682\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Verifying IDs: 100%|██████████| 463/463 [01:26<00:00,  5.37ids/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing IDs after first pass: 0\n",
            "No missing IDs detected.\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}