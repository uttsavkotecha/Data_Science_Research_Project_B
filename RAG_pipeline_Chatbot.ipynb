{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Qdrant and loaded models.\n"
     ]
    }
   ],
   "source": [
    "import os, json, re, time\n",
    "from typing import List, Dict, Any, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Filter, FieldCondition, MatchValue, Range\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import gradio as gr\n",
    "import requests\n",
    "\n",
    "# ------------------ Qdrant Configuration ------------------\n",
    "QDRANT_URL = \"https://20851a9b-65fb-47d0-982e-38fdfc7d76f8.europe-west3-0.gcp.cloud.qdrant.io\"   # Hosted Qdrant instance URL\n",
    "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.ooylH9ScdhcxPm0MywCTUSBDULcNCuuL4U8wd52UwXY\"  # API key for authentication\n",
    "COLLECTION_NAME = \"fda_maude_rag\"          # Collection name in Qdrant\n",
    "VECTOR_NAME = \"foi_embedding\"              # Name of vector field in the collection\n",
    "\n",
    "# ------------------ Embedding & Re-ranking Models ------------------\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"                # Model to create embeddings\n",
    "CROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"           # Cross-encoder for reranking results\n",
    "\n",
    "# ------------------ Generation Settings ------------------\n",
    "USE_OLLAMA = True                                                      # Use local Ollama for generation\n",
    "OLLAMA_HOST = \"http://localhost:11434\"                                 # Ollama local server address\n",
    "OLLAMA_MODEL = \"mistral\"                                               # Model name for Ollama\n",
    "\n",
    "USE_MISTRAL_API = False                                                 # Use cloud Mistral API instead of Ollama\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\", \"YOUR_MISTRAL_KEY\")      # Retrieve API key from environment variables\n",
    "MISTRAL_MODEL = \"mistral-small-latest\"                                 # Cloud Mistral model name\n",
    "\n",
    "# ------------------ Retrieval Parameters ------------------\n",
    "TOP_K = 25             # Number of documents fetched from Qdrant before reranking\n",
    "TOP_N_FINAL = 8        # Number of top reranked documents passed to the generator\n",
    "BLEND_ALPHA = 0.55     # Weighting factor between vector search and reranker scores\n",
    "MAX_CHUNK_CHARS = 1800 # Max characters per document chunk during reranking\n",
    "\n",
    "# ------------------ Initialize Clients & Models ------------------\n",
    "client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)           # Connect to Qdrant instance\n",
    "embedder = SentenceTransformer(EMBED_MODEL)                            # Load sentence transformer for embeddings\n",
    "cross_encoder = CrossEncoder(CROSS_ENCODER_MODEL)                      # Load cross-encoder for reranking\n",
    "\n",
    "# ------------------ Startup Confirmation ------------------\n",
    "print(\"✅ Connected to Qdrant and loaded models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Synonym Dictionaries ------------------\n",
    "# Maps high-level event types to a list of possible synonymous terms\n",
    "EVENT_SYNONYMS = {\n",
    "    \"malfunction\": [\"malfunction\", \"failed\", \"failure\", \"error\", \"fault\", \"broken\"],\n",
    "    \"injury\": [\"injury\", \"harm\", \"bleeding\", \"laceration\"],\n",
    "    \"death\": [\"death\", \"fatal\"],\n",
    "    \"no answer\": [\"no answer\", \"unknown\"],\n",
    "}\n",
    "\n",
    "# Maps brand names to a list of aliases or related product terms\n",
    "BRAND_ALIASES = {\n",
    "    \"dexcom\": [\"dexcom\", \"g6\", \"g7\"],\n",
    "    \"abbott\": [\"abbott\", \"freestyle\", \"libre\"],\n",
    "    \"eversense\": [\"eversense\", \"senseonics\"],\n",
    "}\n",
    "\n",
    "# ------------------ Term Expansion Function ------------------\n",
    "def expand_terms(q: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Given a query string `q`, expand it with synonyms and brand aliases.\n",
    "    Returns a list of unique terms (preserving original order).\n",
    "    \"\"\"\n",
    "    ql = q.lower()      # Convert query to lowercase for case-insensitive matching\n",
    "    out = [q]           # Start with the original query as the first term\n",
    "\n",
    "    # Add event synonyms if the event keyword is found in the query\n",
    "    for k, syns in EVENT_SYNONYMS.items():\n",
    "        if k in ql:\n",
    "            out += syns\n",
    "\n",
    "    # Add brand aliases if the brand name is found in the query\n",
    "    for k, syns in BRAND_ALIASES.items():\n",
    "        if k in ql:\n",
    "            out += syns\n",
    "\n",
    "    # Remove duplicates while preserving the order of terms\n",
    "    seen = set()\n",
    "    res = []\n",
    "    for t in out:\n",
    "        if t not in seen:\n",
    "            res.append(t)\n",
    "            seen.add(t)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# ------------------ Canonical Event Names ------------------\n",
    "# Maps lowercase event keywords to their standardized label form\n",
    "EVENT_CANON = {\n",
    "    \"malfunction\": \"Malfunction\",\n",
    "    \"injury\": \"Injury\",\n",
    "    \"death\": \"Death\",\n",
    "    \"no answer\": \"No Answer\",\n",
    "}\n",
    "\n",
    "# ------------------ Regex Patterns for Event Matching ------------------\n",
    "# Each tuple contains:\n",
    "#   - A regex pattern (case-insensitive match handled by lowercasing query)\n",
    "#   - The standardized event label to assign if matched\n",
    "# Patterns handle:\n",
    "#   - Plural forms (e.g., malfunctions, injuries, deaths)\n",
    "#   - Variants (e.g., injury / injuries)\n",
    "#   - Multi-word forms (e.g., \"no answer\")\n",
    "EVENT_PATTERNS = [\n",
    "    (r\"\\bmalfunction(s)?\\b\", \"Malfunction\"),\n",
    "    (r\"\\binjur(y|ies)\\b\",    \"Injury\"),\n",
    "    (r\"\\bdeath(s)?\\b\",       \"Death\"),\n",
    "    (r\"\\bno\\s+answer\\b\",     \"No Answer\"),\n",
    "]\n",
    "\n",
    "# ------------------ Query Filter Inference ------------------\n",
    "def infer_filters_from_query(q: str) -> dict:\n",
    "    \"\"\"\n",
    "    Given a text query `q`, infer structured filters such as:\n",
    "      - event_type: Based on keyword match with plural-awareness\n",
    "      - year_from, year_to: Based on year mentions in the query\n",
    "      - (optionally) brand hints\n",
    "    Returns a dictionary of inferred filters.\n",
    "    \"\"\"\n",
    "    ql = q.lower()  # Normalize to lowercase for matching\n",
    "    out = {}\n",
    "\n",
    "    # Match event type using regex patterns\n",
    "    for pat, val in EVENT_PATTERNS:\n",
    "        if re.search(pat, ql):\n",
    "            out[\"event_type\"] = val\n",
    "            break  # Stop after first match\n",
    "\n",
    "    # Detect years (supports both 1900s and 2000s) and extract range\n",
    "    years = re.findall(r\"\\b(19\\d{2}|20\\d{2})\\b\", ql)\n",
    "    if years:\n",
    "        ys = sorted({int(y) for y in years})  # Remove duplicates, sort\n",
    "        out[\"year_from\"] = ys[0]              # Earliest year\n",
    "        out[\"year_to\"]   = ys[-1]             # Latest year\n",
    "\n",
    "    # Optional: brand detection logic could go here\n",
    "    # Example:\n",
    "    # if \"eversense\" in ql:\n",
    "    #     out[\"brand\"] = \"eversense sensor\"\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np  # numerical operations (used for log boosting)\n",
    "\n",
    "# ------------------ Tokenizer ------------------\n",
    "def _tok(s: str) -> list:\n",
    "    \"\"\"\n",
    "    Lowercase and tokenize a string into alphanumeric terms.\n",
    "    Returns an empty list if input is None or empty.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return []\n",
    "    return re.findall(r\"[a-z0-9]+\", str(s).lower())\n",
    "\n",
    "# ------------------ Payload Fields to Search ------------------\n",
    "# List of fields in the Qdrant payload to check for keyword matches\n",
    "_PFIELDS = [\n",
    "    \"brand_name\", \"generic_name\", \"manufacturer_d_name\", \"event_type\",\n",
    "    \"model_number\", \"report_number\", \"mdr_report_key\", \"date_received\", \"foi_text\"\n",
    "]\n",
    "\n",
    "# ------------------ Keyword Match Scoring ------------------\n",
    "def _keyword_score(query: str, payload: dict) -> float:\n",
    "    \"\"\"\n",
    "    Compute a keyword match score between the query and a document payload.\n",
    "    - Tokenizes both query and relevant payload fields\n",
    "    - Applies a small term-frequency (TF) log boost for repeated terms\n",
    "    - Normalizes score by query length to avoid bias toward longer queries\n",
    "    \"\"\"\n",
    "    q = Counter(_tok(query))  # token frequency in query\n",
    "    if not q:\n",
    "        return 0.0\n",
    "\n",
    "    hay = []  # all tokens from payload fields\n",
    "    for f in _PFIELDS:\n",
    "        v = payload.get(f)\n",
    "        if v:\n",
    "            hay.extend(_tok(v))\n",
    "\n",
    "    if not hay:\n",
    "        return 0.0\n",
    "\n",
    "    h = Counter(hay)  # token frequency in payload\n",
    "    score = 0.0\n",
    "    for t, w in q.items():\n",
    "        if t in h:\n",
    "            # Match score += query term weight * (1 + log(1 + term freq in payload))\n",
    "            score += w * (1.0 + np.log1p(h[t]))\n",
    "\n",
    "    # Normalize by total query term count\n",
    "    return score / (sum(q.values()) + 1e-6)\n",
    "\n",
    "# ------------------ Blend Weights ------------------\n",
    "# These control the relative importance of different scoring components\n",
    "ALPHA_VEC = 0.45   # weight for vector similarity score from Qdrant\n",
    "ALPHA_RER = 0.35   # weight for cross-encoder (re-ranking) score\n",
    "ALPHA_KEY = 0.20   # weight for keyword-based payload match score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_filter(\n",
    "    brand: Optional[str] = None,\n",
    "    event_type: Optional[str] = None,\n",
    "    adverse_flag: Optional[str] = None,\n",
    "    product_flag: Optional[str] = None,\n",
    "    year_from: Optional[int] = None,\n",
    "    year_to: Optional[int] = None\n",
    ") -> Optional[Filter]:\n",
    "    \"\"\"\n",
    "    Building a Qdrant Filter object based on optional search parameters.\n",
    "\n",
    "    Parameters:\n",
    "        brand         : Filter by device brand name\n",
    "        event_type    : Filter by type of event (e.g., \"Injury\", \"Malfunction\")\n",
    "        adverse_flag  : Filter by adverse event flag value (ignored if 'Any')\n",
    "        product_flag  : Filter by product problem flag value (ignored if 'Any')\n",
    "        year_from     : Lower bound for year (inclusive)\n",
    "        year_to       : Upper bound for year (inclusive)\n",
    "\n",
    "    Returns:\n",
    "        Filter object if any conditions are set, otherwise None.\n",
    "    \"\"\"\n",
    "    must = []  # List of FieldCondition objects that all must be satisfied\n",
    "\n",
    "    # Match brand name if provided\n",
    "    if brand:\n",
    "        must.append(FieldCondition(key=\"brand_name\", match=MatchValue(value=brand)))\n",
    "\n",
    "    # Match event type if provided\n",
    "    if event_type:\n",
    "        must.append(FieldCondition(key=\"event_type\", match=MatchValue(value=event_type)))\n",
    "\n",
    "    # Match adverse event flag (skip if 'Any')\n",
    "    if adverse_flag and adverse_flag not in (\"Any\", \"any\"):\n",
    "        must.append(FieldCondition(key=\"adverse_event_flag\", match=MatchValue(value=adverse_flag)))\n",
    "\n",
    "    # Match product problem flag (skip if 'Any')\n",
    "    if product_flag and product_flag not in (\"Any\", \"any\"):\n",
    "        must.append(FieldCondition(key=\"product_problem_flag\", match=MatchValue(value=product_flag)))\n",
    "\n",
    "    # Match year range if specified\n",
    "    if year_from or year_to:\n",
    "        yr = {}\n",
    "        if year_from:\n",
    "            yr[\"gte\"] = int(year_from)  # greater than or equal to\n",
    "        if year_to:\n",
    "            yr[\"lte\"] = int(year_to)    # less than or equal to\n",
    "        must.append(FieldCondition(key=\"year\", range=Range(**yr)))\n",
    "\n",
    "    # Return a Filter if any conditions exist, otherwise None\n",
    "    return Filter(must=must) if must else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str,\n",
    "             brand=None, event_type=None, adverse_flag=None, product_flag=None,\n",
    "             year_from=None, year_to=None,\n",
    "             top_k: int = TOP_K, top_n_final: int = TOP_N_FINAL):\n",
    "    \"\"\"\n",
    "    Retrieve and rank documents from Qdrant based on a query.\n",
    "\n",
    "    Steps:\n",
    "    1. Optionally infer filters (event type, year range) from the query text.\n",
    "    2. Expand the query with synonyms/aliases for better recall.\n",
    "    3. Encode the query into a vector using the embedding model.\n",
    "    4. Search Qdrant with optional structured filters.\n",
    "    5. If no results and an event type was inferred, retry without the filter (fallback).\n",
    "    6. Rerank results with a cross-encoder + keyword match score.\n",
    "    7. Blend vector, rerank, and keyword scores into a final score.\n",
    "    8. Apply final sorting and optional hard filtering by event type.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- 1. Infer filters from the query (if function available) ----\n",
    "    auto = infer_filters_from_query(query) if 'infer_filters_from_query' in globals() else {}\n",
    "    inferred_event = auto.get(\"event_type\")\n",
    "\n",
    "    # Force event_type from inference if present (e.g., \"malfunctions\", \"injuries\")\n",
    "    if inferred_event:\n",
    "        event_type = inferred_event\n",
    "\n",
    "    # Fill year range from inferred values if not explicitly provided\n",
    "    year_from = year_from if year_from is not None else auto.get(\"year_from\")\n",
    "    year_to   = year_to   if year_to   is not None else auto.get(\"year_to\")\n",
    "\n",
    "    # ---- 2. Expand query with synonyms and aliases ----\n",
    "    q_text = \" \".join(expand_terms(query))\n",
    "\n",
    "    # ---- 3. Encode query into vector form ----\n",
    "    q_vec = embedder.encode([q_text], normalize_embeddings=True)[0].tolist()\n",
    "\n",
    "    # ---- 4. Build Qdrant filter ----\n",
    "    filt = make_filter(brand, event_type, adverse_flag, product_flag, year_from, year_to)\n",
    "\n",
    "    # ---- 5. Initial vector search in Qdrant ----\n",
    "    hits = client.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query_vector={\"name\": VECTOR_NAME, \"vector\": q_vec},\n",
    "        limit=top_k,\n",
    "        with_payload=True,   # include metadata\n",
    "        with_vectors=False,  # we don’t need the stored vectors back\n",
    "        query_filter=filt\n",
    "    )\n",
    "\n",
    "    # ---- Fallback: retry without event filter if nothing found ----\n",
    "    if inferred_event and not hits:\n",
    "        hits = client.search(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            query_vector={\"name\": VECTOR_NAME, \"vector\": q_vec},\n",
    "            limit=top_k,\n",
    "            with_payload=True,\n",
    "            with_vectors=False,\n",
    "            query_filter=None\n",
    "        )\n",
    "\n",
    "    # ---- No results case ----\n",
    "    if not hits:\n",
    "        return []\n",
    "\n",
    "    # ---- 6. Prepare reranking pairs and metadata ----\n",
    "    pairs, meta = [], []\n",
    "    for h in hits:\n",
    "        pl = h.payload or {}\n",
    "        txt = (pl.get(\"foi_text\") or \"\").strip()\n",
    "\n",
    "        # Truncate long text for reranker speed\n",
    "        if len(txt) > MAX_CHUNK_CHARS:\n",
    "            txt = txt[:MAX_CHUNK_CHARS] + \"…\"\n",
    "\n",
    "        pairs.append([q_text, txt if txt else \" \"])\n",
    "        meta.append({\n",
    "            \"id\": h.id,\n",
    "            \"score_vec\": float(h.score),  # original vector similarity score\n",
    "            \"payload\": pl\n",
    "        })\n",
    "\n",
    "    # ---- 7. Rerank results using cross-encoder ----\n",
    "    ce_scores = cross_encoder.predict(pairs).tolist()\n",
    "\n",
    "    # ---- 8. Compute keyword scores and blend final score ----\n",
    "    out = []\n",
    "    for m, ce in zip(meta, ce_scores):\n",
    "        kw = _keyword_score(query, m[\"payload\"])   # keyword match score\n",
    "        ce_norm = 1.0 / (1.0 + np.exp(-ce / 5.0))  # normalize cross-encoder score (sigmoid)\n",
    "        final = (ALPHA_VEC * m[\"score_vec\"]) + (ALPHA_RER * ce_norm) + (ALPHA_KEY * kw)\n",
    "\n",
    "        # Store scoring breakdown for debugging\n",
    "        m[\"score_ce\"] = float(ce)\n",
    "        m[\"score_kw\"] = float(kw)\n",
    "        m[\"score_final\"] = float(final)\n",
    "        out.append(m)\n",
    "\n",
    "    # ---- 9. Sort by final blended score ----\n",
    "    out.sort(key=lambda x: x[\"score_final\"], reverse=True)\n",
    "\n",
    "    # ---- 10. Hard filter by inferred event type if needed ----\n",
    "    if inferred_event:\n",
    "        out = [d for d in out if (d[\"payload\"] or {}).get(\"event_type\") == inferred_event]\n",
    "\n",
    "    # ---- 11. Return top N results ----\n",
    "    return out[:top_n_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure an embedding model is available for semantic grouping\n",
    "try:\n",
    "    embedding_model  # already defined?\n",
    "except NameError:\n",
    "    try:\n",
    "        # If your retrieval uses `embedder`, reuse it to avoid loading twice\n",
    "        if 'embedder' in globals() and embedder is not None:\n",
    "            embedding_model = embedder\n",
    "        else:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to initialize embedding_model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LLM: Ollama Local Configuration =====\n",
    "import os, requests\n",
    "\n",
    "# Enable local Ollama usage; disable remote Mistral\n",
    "USE_OLLAMA = True\n",
    "USE_MISTRAL_API = False  # Ensure cloud Mistral is turned off when using Ollama locally\n",
    "\n",
    "# Local Ollama server configuration (overridable via environment variables)\n",
    "OLLAMA_HOST  = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")  # Local API endpoint\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"mistral\")                # Model name to use (e.g., \"mistral\", \"mistral:latest\", \"llama3\")\n",
    "\n",
    "def call_ollama(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Call a locally hosted Ollama model via HTTP API and return its response.\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The text prompt to send to the model.\n",
    "\n",
    "    Returns:\n",
    "        str: Model's generated text, or an error message if request fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send generation request to Ollama API\n",
    "        r = requests.post(\n",
    "            f\"{OLLAMA_HOST}/api/generate\",\n",
    "            json={\"model\": OLLAMA_MODEL, \"prompt\": prompt, \"stream\": False},\n",
    "            timeout=180,  # Timeout in seconds for long generations\n",
    "        )\n",
    "        r.raise_for_status()  # Raise exception for HTTP errors\n",
    "\n",
    "        # Extract and return the 'response' field from JSON output\n",
    "        return r.json().get(\"response\", \"\")\n",
    "    except Exception as e:\n",
    "        # Return an error message if request or parsing fails\n",
    "        return f\"[Generation error] Ollama request failed: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _llm_generate(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Route a text prompt to the configured LLM backend and return the generated output.\n",
    "\n",
    "    Priority:\n",
    "        1. Local Ollama model (if USE_OLLAMA is True)\n",
    "        2. Remote Mistral API (if USE_MISTRAL_API is True)\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The text input for the language model.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text from the chosen backend, or an error/status message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use local Ollama if enabled\n",
    "        if USE_OLLAMA:\n",
    "            return call_ollama(prompt)\n",
    "\n",
    "        # Use cloud Mistral API if enabled\n",
    "        if USE_MISTRAL_API:\n",
    "            return call_mistral_api(prompt)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Capture and return any generation errors\n",
    "        return f\"[Generation error] {e}\"\n",
    "\n",
    "    # If neither backend is configured, return a notice\n",
    "    return \"[No generation backend configured]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wx/jymrbgr53pj07jxhn81kv9c00000gn/T/ipykernel_72016/332415818.py:200: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=440, label=None, show_copy_button=True, value=[])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://6b28fce633e24fcc35.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6b28fce633e24fcc35.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uttsavkotecha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/var/folders/wx/jymrbgr53pj07jxhn81kv9c00000gn/T/ipykernel_72016/706804267.py:41: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = client.search(\n",
      "/Users/uttsavkotecha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/var/folders/wx/jymrbgr53pj07jxhn81kv9c00000gn/T/ipykernel_72016/706804267.py:41: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = client.search(\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Minimal ChatGPT-style RAG with history\n",
    "# - Single answer style\n",
    "# - History-aware responses\n",
    "# - Sources shown separately in a table (not embedded in answer)\n",
    "# ==========================================\n",
    "import re, time, pandas as pd, gradio as gr\n",
    "from sentence_transformers import util\n",
    "import numpy as np\n",
    "\n",
    "# ---- LLM routing (select backend) ----\n",
    "def _llm_generate(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Sends prompt to the selected LLM backend.\n",
    "    Priority:\n",
    "        1. Ollama local model if USE_OLLAMA is True\n",
    "        2. Remote Mistral API if USE_MISTRAL_API is True\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if USE_OLLAMA:\n",
    "            return call_ollama(prompt)\n",
    "        if USE_MISTRAL_API:\n",
    "            return call_mistral_api(prompt)\n",
    "    except Exception as e:\n",
    "        return f\"[Generation error] {e}\"\n",
    "    return \"[No generation backend configured]\"\n",
    "\n",
    "# ---- Prompt-building configuration ----\n",
    "MAX_TURNS_IN_HISTORY = 6      # Number of recent user/assistant turns to keep for continuity\n",
    "MAX_SNIPPET_CHARS   = 2200    # Approximate token budget for evidence snippets\n",
    "\n",
    "# ---- Compose grouped context from semantically similar docs ----\n",
    "def _compose_context_semantic_grouped(docs, similarity_threshold=0.85):\n",
    "    \"\"\"\n",
    "    Groups semantically similar FOI reports to avoid redundancy before passing to LLM.\n",
    "    Groups are based on cosine similarity between embeddings of FOI text.\n",
    "    \"\"\"\n",
    "    entries = []\n",
    "    # Extract key metadata and FOI text from each doc\n",
    "    for d in docs:\n",
    "        p = d[\"payload\"] or {}\n",
    "        text = (p.get(\"foi_text\") or \"\").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        entries.append({\n",
    "            \"brand\": p.get(\"brand_name\", \"\"),\n",
    "            \"event\": p.get(\"event_type\", \"\"),\n",
    "            \"date\": p.get(\"date_received\", \"\"),\n",
    "            \"text\": text\n",
    "        })\n",
    "\n",
    "    if not entries:\n",
    "        return \"\"\n",
    "\n",
    "    # Encode FOI texts into embeddings\n",
    "    corpus_embeddings = embedding_model.encode(\n",
    "        [e[\"text\"] for e in entries],\n",
    "        convert_to_tensor=True\n",
    "    )\n",
    "\n",
    "    # Group assignments\n",
    "    grouped = []\n",
    "    used = set()\n",
    "    for i in range(len(entries)):\n",
    "        if i in used:\n",
    "            continue\n",
    "        group_indices = [i]\n",
    "        used.add(i)\n",
    "\n",
    "        # Compare entry i to all later entries\n",
    "        for j in range(i + 1, len(entries)):\n",
    "            if j in used:\n",
    "                continue\n",
    "            sim = util.pytorch_cos_sim(corpus_embeddings[i], corpus_embeddings[j]).item()\n",
    "            if sim >= similarity_threshold:\n",
    "                group_indices.append(j)\n",
    "                used.add(j)\n",
    "\n",
    "        grouped.append(group_indices)\n",
    "\n",
    "    # Build text block for each group\n",
    "    parts = []\n",
    "    for idx, group in enumerate(grouped, 1):\n",
    "        brand = entries[group[0]][\"brand\"]\n",
    "        event = entries[group[0]][\"event\"]\n",
    "        dates = sorted({entries[k][\"date\"] for k in group})\n",
    "        detail = entries[group[0]][\"text\"]\n",
    "\n",
    "        parts.append(\n",
    "            f\"[{idx}] brand={brand}, event={event}, dates={', '.join(dates)}\\n{detail}\\n\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "# ---- Build the final prompt for the LLM ----\n",
    "def _build_chat_prompt(history, user_query, docs):\n",
    "    \"\"\"\n",
    "    Builds the full chat prompt:\n",
    "    - System instruction block\n",
    "    - Short conversation history\n",
    "    - Context from retrieved FOI docs (grouped by semantic similarity)\n",
    "    - Current user query\n",
    "    \"\"\"\n",
    "    # Keep last few turns to guide LLM style and context continuity\n",
    "    history_lines = []\n",
    "    for u, b in history[-MAX_TURNS_IN_HISTORY:]:\n",
    "        history_lines.append(f\"User: {u}\")\n",
    "        history_lines.append(f\"Assistant: {b}\")\n",
    "    history_text = \"\\n\".join(history_lines) if history_lines else \"[no prior turns]\"\n",
    "\n",
    "    # Gather grouped evidence snippets\n",
    "    context = _compose_context_semantic_grouped(docs, similarity_threshold=0.85)\n",
    "\n",
    "    # System role and style instructions\n",
    "    sys = (\n",
    "        \"You are an academic researcher specializing in medical device safety analysis. \"\n",
    "        \"Write responses in a formal, evidence-based, and professional tone suitable for an academic or regulatory report. \"\n",
    "        \"Summarize findings using precise, objective language, avoiding conversational expressions. \"\n",
    "        \"Organize the output into a 'Summary' section followed by a 'Key Reports' section with enumerated points. \"\n",
    "        \"Each key report should clearly state the device, event type, and date(s), followed by a concise description of the incident. \"\n",
    "        \"Do NOT include a 'Sources' section, document IDs, or any reference numbers in the answer. \"\n",
    "        \"If the evidence is insufficient, state this formally.\"\n",
    "    )\n",
    "\n",
    "    # Final assembled prompt\n",
    "    prompt = (\n",
    "        f\"{sys}\\n\\n\"\n",
    "        f\"Conversation so far:\\n{history_text}\\n\\n\"\n",
    "        f\"Evidence snippets:\\n{context}\\n\\n\"\n",
    "        f\"User: {user_query}\\n\"\n",
    "        f\"Assistant:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# ---- Create sources table from retrieved docs ----\n",
    "def _sources_table(docs):\n",
    "    \"\"\"Create a DataFrame of source metadata for display below the chat.\"\"\"\n",
    "    rows = []\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        p = d[\"payload\"] or {}\n",
    "        rows.append([\n",
    "            i,\n",
    "            p.get(\"brand_name\", \"\"),\n",
    "            p.get(\"event_type\", \"\"),\n",
    "            p.get(\"date_received\", \"\"),\n",
    "            p.get(\"report_number\", p.get(\"mdr_report_key\", \"\")),\n",
    "            f\"{d.get('score_final', 0.0):.3f}\"\n",
    "        ])\n",
    "    return pd.DataFrame(rows, columns=[\"#\", \"Brand\", \"Event\", \"Date\", \"Report\", \"Score\"])\n",
    "\n",
    "# ---- Strip any model-added 'Sources' text from answer ----\n",
    "def _strip_sources_block(text: str) -> str:\n",
    "    \"\"\"Remove any 'Sources:' section the model might have added.\"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    m = re.search(r\"\\n\\s*Sources:\\s*.*\", text, flags=re.I | re.S)\n",
    "    return text[:m.start()].rstrip() if m else text\n",
    "\n",
    "# ---- One conversational turn ----\n",
    "TOP_K = 25          # How many docs to retrieve from vector DB\n",
    "TOP_N_FINAL = 8     # How many to keep after reranking\n",
    "\n",
    "def chat_turn(user_msg, history):\n",
    "    \"\"\"\n",
    "    Process a single chat turn:\n",
    "    - Retrieve relevant docs\n",
    "    - Build and send prompt to LLM\n",
    "    - Return updated chat history and sources table\n",
    "    \"\"\"\n",
    "    # If user input is empty, return current state with empty sources\n",
    "    if not user_msg or not user_msg.strip():\n",
    "        empty_tbl = pd.DataFrame(columns=[\"#\", \"Brand\", \"Event\", \"Date\", \"Report\", \"Score\"])\n",
    "        return history, empty_tbl, history\n",
    "\n",
    "    # Retrieve docs relevant to query\n",
    "    docs = retrieve(user_msg, top_k=TOP_K, top_n_final=TOP_N_FINAL)\n",
    "    if not docs:\n",
    "        bot = \"I couldn’t find anything clearly relevant. Try adding a brand, model, or year (e.g., 'Dexcom G6 malfunctions 2020').\"\n",
    "        new_hist = history + [(user_msg, bot)]\n",
    "        empty_tbl = pd.DataFrame(columns=[\"#\", \"Brand\", \"Event\", \"Date\", \"Report\", \"Score\"])\n",
    "        return new_hist, empty_tbl, new_hist\n",
    "\n",
    "    # Build final prompt for LLM and generate answer\n",
    "    prompt = _build_chat_prompt(history, user_msg, docs)\n",
    "    answer = _strip_sources_block(_llm_generate(prompt))\n",
    "\n",
    "    # Create table of sources\n",
    "    table = _sources_table(docs)\n",
    "\n",
    "    # Update conversation history\n",
    "    new_hist = history + [(user_msg, answer)]\n",
    "    return new_hist, table, new_hist\n",
    "\n",
    "# ---- Gradio UI setup ----\n",
    "with gr.Blocks(title=\"FDA Medical Device Chatbot\") as chat_demo:\n",
    "    gr.Markdown(\"### FDA Medical Device Chatbot\")\n",
    "    state = gr.State([])  # Holds conversation history [(user, bot), ...]\n",
    "\n",
    "    # Main chat window\n",
    "    chatbot = gr.Chatbot(height=440, label=None, show_copy_button=True, value=[])\n",
    "\n",
    "    # Input row\n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(placeholder=\"Ask anything about MAUDE adverse events…\", scale=6)\n",
    "        send = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "        clear = gr.Button(\"Clear\")\n",
    "\n",
    "    # Sources table display\n",
    "    sources = gr.Dataframe(\n",
    "        headers=[\"#\", \"Brand\", \"Event\", \"Date\", \"Report\", \"Score\"],\n",
    "        interactive=False,\n",
    "        wrap=True,\n",
    "        label=\"Sources (last answer)\"\n",
    "    )\n",
    "\n",
    "    # Send button click handler\n",
    "    send.click(\n",
    "        fn=chat_turn,\n",
    "        inputs=[msg, state],\n",
    "        outputs=[chatbot, sources, state],  # Writes back updated history to state\n",
    "    ).then(lambda: \"\", None, msg)  # Clears input box after sending\n",
    "\n",
    "    # Pressing Enter in the textbox triggers same as send\n",
    "    msg.submit(\n",
    "        fn=chat_turn,\n",
    "        inputs=[msg, state],\n",
    "        outputs=[chatbot, sources, state],\n",
    "    ).then(lambda: \"\", None, msg)\n",
    "\n",
    "    # Clear chat button handler\n",
    "    def _clear():\n",
    "        \"\"\"Reset chat, sources table, and state.\"\"\"\n",
    "        return [], pd.DataFrame(columns=[\"#\", \"Brand\", \"Event\", \"Date\", \"Report\", \"Score\"]), []\n",
    "    clear.click(_clear, inputs=None, outputs=[chatbot, sources, state])\n",
    "\n",
    "# Launch Gradio app (share=True for public link)\n",
    "chat_demo.launch(share=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bertscore\n",
    "\n",
    "smooth = SmoothingFunction().method4\n",
    "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "def _gen_answer_with_latency(query: str):\n",
    "    t0 = time.time()\n",
    "    docs = retrieve(query, top_k=TOP_K, top_n_final=TOP_N_FINAL)\n",
    "    t_retrieve = time.time() - t0\n",
    "    if not docs:\n",
    "        return \"\", t_retrieve, 0.0\n",
    "    prompt = _build_chat_prompt([], query, docs)\n",
    "    t1 = time.time()\n",
    "    out = _llm_generate(prompt).strip()\n",
    "    t_gen = time.time() - t1\n",
    "    return out, t_retrieve, t_gen\n",
    "\n",
    "def _bleu(ref: str, hyp: str):\n",
    "    if not ref or not hyp:\n",
    "        return np.nan\n",
    "    return sentence_bleu([ref.split()], hyp.split(), smoothing_function=smooth)\n",
    "\n",
    "def _rouge_l(ref: str, hyp: str):\n",
    "    if not ref or not hyp:\n",
    "        return np.nan\n",
    "    return rouge.score(ref, hyp)[\"rougeL\"].fmeasure\n",
    "\n",
    "def _bertscore_f1(ref: str, hyp: str):\n",
    "    if not ref or not hyp:\n",
    "        return np.nan\n",
    "    P, R, F1 = bertscore([hyp], [ref], lang=\"en\")\n",
    "    return float(F1.mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_items = [\n",
    "    {\n",
    "        \"query\": \"Events where Death Occured\",\n",
    "        \"reference\": \"\"\"Summary:\n",
    "The analysis focused on six events involving medical devices that resulted in death, spanning from 2020 to 2021. The devices include True Metrix and Dexcom G6 continuous glucose monitoring systems. The evidence gathered indicates that the root causes of these incidents are not fully determined due to insufficient information about the underlying conditions and potential contributing factors.\n",
    "\n",
    "Key Reports:\n",
    "1. Device: True Metrix, Event Type: Death, Date(s): 2021-07-23\n",
    "   - The customer's deceased husband had used a True Metrix meter prior to his death. The customer reported issues with the meter but declined further investigation or blood tests. No cause of death was disclosed for either the customer or her husband.\n",
    "\n",
    "2. Device: G6 Continuous Glucose Monitoring System, Event Type: Death, Date(s): 2020-12-15\n",
    "   - The patient's relatives were unable to be reached, and the cause of death was unknown to both the patient's doctor and primary care physician.\n",
    "\n",
    "3. Device: Dexcom G6 Continuous Glucose Monitoring System, Event Type: Death, Date(s): 2020-07-01\n",
    "   - Signal loss over one hour was reported, but no product or data was provided for evaluation. The cause of death and a probable cause could not be determined as no injury or medical intervention was reported.\n",
    "\n",
    "4. Device: True Metrix Air, Event Type: Death, Date(s): 2021-10-22\n",
    "   - The customer passed away with a high blood glucose level (600 mg/dl), but the fasting/non-fasting status was not disclosed. The death was likely related to the patient being a terminal diabetic, according to their caregiver. No product or data were returned for evaluation.\n",
    "\n",
    "5. Device: Dexcom G6 Continuous Glucose Monitoring System, Event Type: Death, Date(s): 2020-07-01, 2021-07-30\n",
    "   - Insufficient information was provided to determine the cause of death or the role of the device in these incidents.\n",
    "\n",
    "6. Device: True Metrix, Event Type: Death, Date(s): 2021-07-23\n",
    "   - The customer reported issues with recharging their deceased husband's True Metrix meter, but no further investigation or details were provided. No cause of death was disclosed for the customer or her husband.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Adverse Events in Glucose Monitoring systems\",\n",
    "        \"reference\": \"\"\"Summary:\n",
    "This report presents an analysis of adverse events associated with the Dexcom G6 Continuous Glucose Monitoring System (CGMS). Between 2019 and 2021, multiple incidents were reported where the CGMS readings differed from those obtained by a blood glucose meter. The events occurred on specific dates mentioned above. The discrepancy in values fell within the B Zone of the Parkes Error Grid, indicating moderate inaccuracies, although no injuries or medical interventions were reported.\n",
    "\n",
    "Key Reports:\n",
    "1. Device: Dexcom G6 CGMS, Event Type: Malfunction, Date(s): 2019-09-11, 2020-09-25, 2020-10-05, 2020-11-12, 2021-01-13, 2021-03-02, 2021-08-03, 2021-10-07\n",
    "   Description: Inaccuracies between the CGMS and blood glucose meter were reported on several occasions. The sensor was inserted off-label into the arm on 09/02/2020, and data investigation confirmed the allegation of discrepancies. However, the cause could not be determined via data analysis. The reported glucose values fell within the D Zone of the Parkes Error Grid but were found to be different enough to fall within the B Zone. No injury or medical intervention was required.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Malfunctions eversense sensor devices in 2019\",\n",
    "        \"reference\": \"\"\"Summary:\n",
    "The review of available evidence reveals multiple reported malfunction incidents involving the Eversense sensor device during the year 2019. The specific issues encountered included difficulties with explantation, which required more than one attempt to remove the sensor.\n",
    "\n",
    "Key Reports:\n",
    "1. Device: Eversense Sensor\n",
    "   Event Type: Malfunction - Explantation Issue\n",
    "   Date(s): August 30, September 13, October 14 (2019)\n",
    "   Description: In one incident, a healthcare professional encountered difficulty in explanting the Eversense sensor on the first attempt made on August 30. The sensor was successfully removed during the second attempt made on October 14.\n",
    "\n",
    "2. Device: Eversense Sensor\n",
    "   Event Type: Malfunction - Explantation Issue\n",
    "   Date(s): August 2, September 18, October 9, October 14 (2019)\n",
    "   Description: In another incident, a healthcare professional faced difficulties in explanting the Eversense sensor on the first attempt. The exact date of the initial attempt and subsequent removal is not specified in the available data.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Injurys in libre Devices in 2021\",\n",
    "        \"reference\": \"\"\"Summary:\n",
    "\n",
    "This analysis focuses on adverse events related to the use of Freestyle Libre glucose monitoring systems during the year 2021. A total of five incidents were identified, all categorized as injuries. The incidents involved two different models: Freestyle Libre 14-day and Freestyle Libre 2.\n",
    "\n",
    "Key Reports:\n",
    "\n",
    "1. Event: Injury related to Freestyle Libre 14-day device\n",
    "   - Date(s): March 8, 2021, December 29, 2021\n",
    "   - Description: Upon removal of the sensor filament remained in the customer's skin, causing pain. No further information was provided regarding the extent or duration of the injury.\n",
    "\n",
    "2. Event: Injury related to Freestyle Libre 2 device\n",
    "   - Date: February 25, 2021\n",
    "   - Description: The applicator needle got stuck in the sensor during application, causing \"hemorrhage\". The needle had to be surgically removed due to threatening bleeding. No further treatment was reported.\n",
    "\n",
    "3. Event: Injury related to Freestyle Libre 14-day device\n",
    "   - Date: June 8, 2021\n",
    "   - Description: Insertion issue resulted in a broken sensor filament that remained in the customer's arm, causing pain, swelling, and bleeding. A healthcare provider removed the sensor filament, and no further treatment was required.\n",
    "\n",
    "4. Event: Injury related to Freestyle Libre 2 device\n",
    "   - Date: October 18, 2021\n",
    "   - Description: Bleeding occurred after sensor insertion, which continued into the next day. A healthcare professional attempted to stop bleeding with a pressure bandage, but this was not effective. The wound was then cauterized with silver nitrate. No further treatment or medication was required.\n",
    "\n",
    "5. Event: Injury related to Freestyle Libre 14-day device\n",
    "   - Date: February 1, 2021\n",
    "   - Description: A caregiver reported heavy bleeding while a customer was wearing the sensor. The customer lost consciousness but was able to self-treat once they regained consciousness. No further information was provided regarding the extent or duration of the injury.\"\"\"\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]/Users/uttsavkotecha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/var/folders/wx/jymrbgr53pj07jxhn81kv9c00000gn/T/ipykernel_72016/706804267.py:41: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = client.search(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/uttsavkotecha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Evaluating:  25%|██▌       | 1/4 [01:38<04:56, 98.93s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating:  50%|█████     | 2/4 [02:30<02:22, 71.03s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating:  75%|███████▌  | 3/4 [03:21<01:02, 62.10s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|██████████| 4/4 [04:28<00:00, 67.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro averages\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bleu</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>bertscore_f1</th>\n",
       "      <th>latency_retrieve_s</th>\n",
       "      <th>latency_generate_s</th>\n",
       "      <th>latency_total_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>macro_avg</th>\n",
       "      <td>0.272436</td>\n",
       "      <td>0.539485</td>\n",
       "      <td>0.92208</td>\n",
       "      <td>6.428841</td>\n",
       "      <td>53.47934</td>\n",
       "      <td>59.908182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               bleu    rougeL  bertscore_f1  latency_retrieve_s  \\\n",
       "macro_avg  0.272436  0.539485       0.92208            6.428841   \n",
       "\n",
       "           latency_generate_s  latency_total_s  \n",
       "macro_avg            53.47934        59.908182  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-query scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>bleu</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>bertscore_f1</th>\n",
       "      <th>ref_len</th>\n",
       "      <th>hyp_len</th>\n",
       "      <th>latency_retrieve_s</th>\n",
       "      <th>latency_generate_s</th>\n",
       "      <th>latency_total_s</th>\n",
       "      <th>hyp_preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Events where Death Occured</td>\n",
       "      <td>0.241838</td>\n",
       "      <td>0.475436</td>\n",
       "      <td>0.904291</td>\n",
       "      <td>328</td>\n",
       "      <td>275</td>\n",
       "      <td>4.520098</td>\n",
       "      <td>87.343865</td>\n",
       "      <td>91.863963</td>\n",
       "      <td>Summary: This analysis presents five incidents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adverse Events in Glucose Monitoring systems</td>\n",
       "      <td>0.292434</td>\n",
       "      <td>0.564767</td>\n",
       "      <td>0.933937</td>\n",
       "      <td>168</td>\n",
       "      <td>180</td>\n",
       "      <td>6.475954</td>\n",
       "      <td>36.342108</td>\n",
       "      <td>42.818062</td>\n",
       "      <td>Summary: The analysis focuses on adverse event...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Malfunctions eversense sensor devices in 2019</td>\n",
       "      <td>0.285814</td>\n",
       "      <td>0.569659</td>\n",
       "      <td>0.928134</td>\n",
       "      <td>148</td>\n",
       "      <td>173</td>\n",
       "      <td>7.136031</td>\n",
       "      <td>37.834176</td>\n",
       "      <td>44.970207</td>\n",
       "      <td>Summary: The analysis of incidents involving t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Injurys in libre Devices in 2021</td>\n",
       "      <td>0.269655</td>\n",
       "      <td>0.548077</td>\n",
       "      <td>0.921957</td>\n",
       "      <td>296</td>\n",
       "      <td>321</td>\n",
       "      <td>7.583283</td>\n",
       "      <td>52.397212</td>\n",
       "      <td>59.980495</td>\n",
       "      <td>Summary: The analysis of incident reports from...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           query      bleu    rougeL  \\\n",
       "0                     Events where Death Occured  0.241838  0.475436   \n",
       "1   Adverse Events in Glucose Monitoring systems  0.292434  0.564767   \n",
       "2  Malfunctions eversense sensor devices in 2019  0.285814  0.569659   \n",
       "3               Injurys in libre Devices in 2021  0.269655  0.548077   \n",
       "\n",
       "   bertscore_f1  ref_len  hyp_len  latency_retrieve_s  latency_generate_s  \\\n",
       "0      0.904291      328      275            4.520098           87.343865   \n",
       "1      0.933937      168      180            6.475954           36.342108   \n",
       "2      0.928134      148      173            7.136031           37.834176   \n",
       "3      0.921957      296      321            7.583283           52.397212   \n",
       "\n",
       "   latency_total_s                                        hyp_preview  \n",
       "0        91.863963  Summary: This analysis presents five incidents...  \n",
       "1        42.818062  Summary: The analysis focuses on adverse event...  \n",
       "2        44.970207  Summary: The analysis of incidents involving t...  \n",
       "3        59.980495  Summary: The analysis of incident reports from...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = []\n",
    "for item in tqdm(test_items, desc=\"Evaluating\"):\n",
    "    q = item[\"query\"].strip()\n",
    "    ref = (item.get(\"reference\") or \"\").strip()\n",
    "\n",
    "    hyp, t_retrieve, t_gen = _gen_answer_with_latency(q)\n",
    "\n",
    "    rows.append({\n",
    "        \"query\": q,\n",
    "        \"bleu\": _bleu(ref, hyp),\n",
    "        \"rougeL\": _rouge_l(ref, hyp),\n",
    "        \"bertscore_f1\": _bertscore_f1(ref, hyp),\n",
    "        \"ref_len\": len(ref.split()),\n",
    "        \"hyp_len\": len(hyp.split()),\n",
    "        \"latency_retrieve_s\": t_retrieve,\n",
    "        \"latency_generate_s\": t_gen,\n",
    "        \"latency_total_s\": t_retrieve + t_gen,\n",
    "        \"hyp_preview\": hyp[:220].replace(\"\\n\", \" \") + (\"…\" if len(hyp) > 220 else \"\")\n",
    "    })\n",
    "\n",
    "df_gen = pd.DataFrame(rows)\n",
    "\n",
    "macro = df_gen[[\"bleu\",\"rougeL\",\"bertscore_f1\",\"latency_retrieve_s\",\"latency_generate_s\",\"latency_total_s\"]].mean().to_frame(\"mean\").T\n",
    "macro.index = [\"macro_avg\"]\n",
    "\n",
    "print(\"Macro averages\")\n",
    "display(macro)\n",
    "\n",
    "print(\"Per-query scores\")\n",
    "display(df_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
